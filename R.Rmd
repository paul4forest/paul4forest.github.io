---
title: "R Commands"
author: "Paul Rougieux"
date: "17 August 2017"
output: 
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

This page is the continuation of [my blog post on R commands](https://paulremote.blogspot.de/2014/04/r-commands.html). 
On the blog, see also [why use R](http://paulremote.blogspot.fr/2013/10/why-r.html) and the [RSS feed of posts labelled R](http://paulremote.blogspot.com/feeds/posts/default/-/R).

See also documentation at:

 * The Comprehensive R Archive Network [manuals](https://cran.r-project.org/manuals.html)
 * Tidyverse [packages](http://www.tidyverse.org/packages/)

# Installing R and packages

To install R and Rstudio on Debian, see the 
[debian.html#r_and_rstudio](debian.html#r_and_rstudio) page on this site.

To install packages simply enter the following at an R command prompt:

    install.packages("package_name")

Some packages have dependencies that need to be installed at the OS level.
Error messages:

    "Configuration failed because libcurl was not found." 
    "Configuration failed because libxml-2.0 was not found."
    "Configuration failed because openssl was not found."

Can be solved by installing these dependencies: 

    sudo apt install libcurl4-openssl-dev 
    sudo apt install libxml2-dev 
    sudo apt install libssl-dev

# Information about your R system

```{r eval=FALSE}
sessionInfo()
installed.packages()
```

# Files input output

    getwd()
    list.files(tempdir()) 
    dir.create("blabla")

## CSV files

Read one csv file with default R function.

    read.csv("data.csv", )

Read many csv files with functions from the tidyverse packages.

First write sample csv files to a temporary directory.
It's more complicated than I thought it would be.

    data_folder <- file.path(tempdir(), "iris")
    dir.create(data_folder)
    iris %>%
        # Keep the Species column in the output
        # Create a new column that will be used as the grouping variable
        mutate(species_group = Species) %>% 
        group_by(species_group) %>%
        nest() %>%
        by_row(~write.csv(.$data, 
                          file = file.path(data_folder, paste0(.$species_group, ".csv")),
                          row.names = FALSE))


Read these csv files into one data frame.
Note the Species column has to be present in the csv files, otherwise we would loose that information. 

    iris_csv <- list.files(data_folder, full.names = TRUE) %>% 
        map_dfr(read_csv)

`write_csv` returned an `Error in write_delim(...) is.data.frame(x) is not TRUE`
That's why we used `write.csv` instead.

# Find

## Longest contiguous stretch of non-NAs:

    na.contiguous(c(NA,1:3,NA,NA,3:5,NA,NA))
    ] 1 2 3
    tr(,"na.action")
    ]  1  5  6  7  8  9 10 11
    tr(,"class")
    ] "omit"
    tr(,"tsp")
    ] 2 4 1
    na.contiguous(c(NA,1:3,NA,NA,3:6,NA,NA))
    ] 3 4 5 6
    tr(,"na.action")
    ]  1  2  3  4  5  6 11 12
    tr(,"class")
    ] "omit"
    tr(,"tsp")
    ]  7 10  1


# Vectors

[SO answer concerning indexing up to end of vector/matrix](https://stackoverflow.com/a/7500961/2641825) "Sometimes it's easier to tell R what you don't want"

    x <- c(5,5,4,3,2,1)
    x[-(1:3)]
    x[-c(1,3,6)]

## Set operations

```{r}
x = letters[1:3]
y = letters[3:5]
union(x, y)
intersect(x, y)
setdiff(x, y)
setdiff(y, x)
setequal(x, y)
```

# Lists
Given a list structure x, unlist simplifies it to produce a vector which contains all the atomic components which occur in x.
```{r}
l1 <- list(a="a", b="2,", c="pi+2i")
str(l1)
unlist(l1) # a character vector 
str(unlist(l1))
```

# Strings

```{r}
message("Using the following letters: ", paste(letters, collapse=","), ".")
```

## Levenshtein distance between words

Cf. https://en.wikipedia.org/wiki/Levenshtein_distance

    adist("kitten", "sitting")

# S3 methods
x<-1



List all available methods for a class:

    methods(class="lm") 


# One liners

Remove all objects in the workspace except one :

    rm(list=ls()[!ls()=="object_to_keep"]) 
    rm(list=ls()[!ls()=="con"]) # Remove all except a database connection


# knitr


## Cross references

[R markdown cross
ref](https://bookdown.org/yihui/rmarkdown-cookbook/cross-ref.html)

> "Cross-referencing is not provided directly within the base rmarkdown
> package, but is provided as an extension in bookdown (Xie 2020c). We must
> therefore use an output format from bookdown (e.g., html_document2,
> pdf_document2, and word_document2, etc.) in the YAML output field."

* Bookdown issue with flextable [Table and figure cross references in word](https://github.com/rstudio/bookdown/issues/746)
Now solved with the following implementation

```{r irishead, ft.align="left", tab.cap="Edgar Anderson's Iris Data", eval=FALSE}
iris %>% head() %>% flextable()
```
This is a reference to flextable \@ref(tab:irishead).
The chunk name should not contain an underscore, otherwise the cross reference
will not work. See [bookdown issue 941](https://github.com/rstudio/bookdown/issues/941).

See more flextable caption examples in `system.file(package = "flextable", "examples", "rmd", "captions")`


## kable to create tables
```{r }
cat(kable(head(iris, 1), format = "html"))
cat(kable(head(iris, 1), format = "latex"))
cat(kable(head(iris, 1), format = "markdown"))
```

## Setting knitr options
Those 2 commands are different.
Sets the options for chunk, within a knitr chunk inside the .Rmd document

    opts_chunk$set(fig.width=10)

 Sets the options for knitr outside the .Rmd document

    opts_knit$set()

## R markdown python engine

Rstudio: [R Mardown python engine](https://rstudio.github.io/reticulate/articles/r_markdown.html)


## Flextable and officer package

The officer package makes it possible to create improved word and powerpoint
documents. The flextable package has a few functions to generate tables for
word documents (otherwise they look funny).

Before installing flextable, the following system dependencies were required:

    sudo apt install libcairo2-dev libjpeg-dev libgif-dev

Installation in R:

    install.packages("flextable")

This is to work with colleagues who are not in the latex/pdf world.

### Landscape pages
```{r insert_landscape_section, eval=FALSE}
library(officer)
library(flextable)
ft_iris <- iris %>%
    head() %>%
    flextable() %>%
    autofit()
# Add a flex table in landscape format 
# Supply document name here, otherwise start an empty doc
doc <- read_docx() %>% 
  # Portait and landscape sections are defined by their ending
  # A portrait section is ending here
  body_end_section_portrait() %>%
  body_add_flextable(value = ft_iris, split = TRUE) %>%
  # A landscape section is ending here
  body_end_section_landscape() %>% 
  print(target = "/tmp/iris.docx")
```

# Rscript
Capture arguments in an Rscript on windows and write them to a file
```
"C:\Program Files\R\R-3.5.0\bin\Rscript.exe" --verbose -e "args = commandArgs(trailingOnly=TRUE)" -e "writeLines(args,'C:\\Dev\\args.txt')" "file1.csv" "file2.csv" "file3.csv"
```
Arguments can be extracted one by one with args[1]
commandArgs() returns a **character vector** containing the name of the executable and the user-supplied command line arguments.

# Tidyverse


## dplyr
pipes

```{r warning=FALSE, message=FALSE}
library(dplyr)
cars %>%
    group_by(speed) %>%
    print() %>% # works because the print function returns its argument
    summarise(numberofcars = n(),
              min = min(dist),
              mean = mean(dist),
              max = max(dist)) 
```



group_by() creates a tbl_df objects which is a wrapper around a data.frame to enable some functionalities. Note that print returns its output on a tbl_df object. So print() can be used inside the pipe without stopping the workflow.


### Mutate

Mutate multiple variables in the dataframe at once using the `vars()` helper function to scope the mutation:
```{r} 
iris %>%
    mutate_at(vars(starts_with("Petal")), round) %>%
    head()
```

## purrr

Hadley Wickham's answer to a SO question 
[Why use purrr::map instead of lapply?](https://stackoverflow.com/a/47123420/2641825)


### Map a function to nested data sets

Load data

```{r eval=FALSE}
list.files(getwd())
forestEU_wide <- read.csv("Forest-R-EU.csv", stringsAsFactors = FALSE)
head(forestEU)
```
Pivot to long format
```{r eval=FALSE}
# at some pointin the future this will be called pivot_long
forestEU <- forestEU_wide  %>% 
      # select everything except the Year, then pivot all columns and put the value in area
      gather(-Year, key = "Country", value = "Area")
```

## Tricks

David Robinson [Ten tricks in the tidyverse](https://www.youtube.com/watch?v=NDHSBUN_rVU)

1. count with weight, sort and renamining 
   count(x, ..., wt = NULL, sort = FALSE, name = "n")
2. creating variables in count()
3. add_count()
4. summarize with a list column (to create many models for example)
5. fct_reorder() + geom_col() + coord_flip()
6. fct_lump() to lump less frequent values together
7. use a log scale scale_x/y_log10
8. crossing()
9. separate()
10. extract() use a regular expression such as `S(.*)E(.*)` to extract series
    and episode from a string of the form "S32E44".

# Interpolate for one country
```{r eval=FALSE}
country_interpolation <- function(df) {
      df <- data.frame(approx(df$Year, df$Area, method = "linear", n=71))
  df <- rename(df, Year = x, Area = y)
    return(df)
}

forestEU %>%  filter(Country=="Austria") %>% country_interpolation()
```


Interpolate for all countries
Perform the Interpolation on all countries


See documentation in 
* many models https://r4ds.had.co.nz/many-models.html
* blog https://emoriebeck.github.io/R-tutorials/purrr/


```{r eval=FALSE}

forestEU_nested <- forestEU %>%
      # Remove empty area
      filter(!is.na(Area)) %>% 
      group_by(Country) %>%  
      nest() %>% 
      mutate(interpolated = map(data, country_interpolation))

# forestEU_nested %>% unnest(data)
# Unnest the interpolated data to look at it and plotting
forestEU_interpolated <- forestEU_nested %>% unnest(interpolated)
```


# Write to many csv
```{r eval=FALSE}
#dir.create("output")
forestEU_nested <- forestEU_nested %>% 
    mutate(filename = paste0("output/", Country, ".csv"),
    wrote_stuff = map2(interpolated, filename, write.csv))
```


## tidy evaluation

* Advanced R [metaprogramming section](https://adv-r.hadley.nz/meta.html)
* [Examples of tidy evaluation with plots](https://www.tidyverse.org/articles/2018/07/ggplot2-tidy-evaluation/)

```{r eval = FALSE}
scatter_plot <- function(data, x, y) {
    x <- enquo(x)
    y <- enquo(y)
    ggplot(data) + geom_point(aes(!!x, !!y))
}
scatter_plot(mtcars, disp, drat)
```

Another example use of metaprogramming to change variables
```{r eval = FALSE}
add1000 <- function(dtf, var){
      varright <- enquo(var)
  varleft <- quo_name(enquo(var))
    dtf %>% 
            mutate(!!varleft := 1000 + (!!varright))
}
add1000(iris, Sepal.Length)
```


* [Tidy evaluation explained in French](https://thinkr.fr/tidyeval/)

## tidyr

[tidyr vignette on tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) 
In the section on "Multiple types in one table":

> Datasets often involve values collected at multiple levels, on different types of observational units. During tidying, each type of observational unit should be stored in its own table. This is closely related to the idea of database normalisation, where each fact is expressed in only one place. It’s important because otherwise inconsistencies can arise.

> Normalisation is useful for tidying and eliminating inconsistencies. However, there are few data analysis tools that work directly with relational data, so analysis usually also requires denormalisation or the merging the datasets back into one table.

Example use of `tidyr::nest()` to generate a group of plots: 
[make ggplot2 purrr](http://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/).

```{r eval=FALSE}
library(tidyr)
library(dplyr)
library(purrr)
library(ggplot2)
piris <- iris %>%  
    group_by(Species) %>% 
    nest() %>% 
    mutate(plot = map2(data, Species, 
                       ~ggplot(data = .x, 
                               aes(x = Petal.Length, y = Petal.Width)) + 
                           geom_point() + ggtitle(.y)))
piris$plot[1]
piris$plot[3]
piris$plot[2]
```

# Discussions

* Gavin Simpson [My aversion to pipers](http://www.fromthebottomoftheheap.net/2015/06/03/my-aversion-to-pipes/) shows an Hadley tweet explaining that pipe might not be good in package development.

## Stackoverflow

* [Stack overflow question
  checklist](https://codeblog.jonskeet.uk/2012/11/24/stack-overflow-question-checklist/)
  general programming
* [How to make a great R reproducible
  example](https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example) 


# Teaching R

* [Teach the tidyverse to beginners](http://varianceexplained.org/r/teach-tidyverse/)

* [Vectorization in R, Why?](http://www.noamross.net/blog/2014/4/16/vectorization-in-r--why.html)

* [Recommendations for resources for after workshops](https://github.com/carpentries/conversations/issues/18)


# Plots


## Plotting with ggplot2

    geom_bar
    geom_tile + a gradient produce heat maps

## Choropleth maps

[Choropleth map](https://www.r-graph-gallery.com/choropleth-map.html)

Shape files to make choropleth maps of Europe:
[GISCO: Geographical Information and maps](https://ec.europa.eu/eurostat/web/gisco/geodata/reference-data/administrative-units-statistical-units/nuts)



## Palettes

[Setting up colour palettes in R](https://data.library.virginia.edu/setting-up-color-palettes-in-r/)

> To create a RColorBrewer palette, use the brewer.pal function. It takes two
> arguments: n, the number of colors in the palette; and name, the name of the
> palette. Let’s make a palette of 8 colors from the qualitative palette,
> “Set2”. 


    library(RColorBrewer)
    brewer.pal(n = 8, name = "Set2") 
    [1] "#66C2A5" "#FC8D62" "#8DA0CB" "#E78AC3"
    "#A6D854" "#FFD92F" "#E5C494" "#B3B3B3" 
    palette(brewer.pal(n = 8, name = "Set2"))

Use this palette in ggplot2

    ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) + 
        geom_point() +
        scale_color_brewer(palette = "Set2")


Use a named vector to set a palette in ggplot2 as explained in 
[ggplot2 scale_manual](https://ggplot2.tidyverse.org/reference/scale_manual.html)

    p <- ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, color=Species)) +
            geom_point() 
    p + scale_colour_manual(values = c(setosa='black', versicolor='red', virginica='green'))

Create a named palette using R colour brewer:

    species_names <- c("setosa", "versicolor", "virginica") 
    iris_palette <- setNames(brewer.pal(n=length(species_names), name='Set2'), 
                             species_names)
    p + scale_colour_manual(values = iris_palette)

### Display palettes

Display qualitative palettes:

    display.brewer.all(type="qual") 

Display all palettes 

    display.brewer.all()

# Creating a package
You might want to read the CRAN manual on [Writing R Extensions](https://cran.r-project.org/doc/manuals/R-exts.html), and its section on [Package dependencies](https://cran.r-project.org/doc/manuals/R-exts.html#Package-Dependencies).
See also Hadley's book on R package and its section on [Namespace](http://r-pkgs.had.co.nz/namespace.html)

Use the devtools library to start a package folder structure:

    devtools::create("package_name")

Use git to track code modifications (shell commands):

    $ cd package_name
    $ git init

## Documentation

The roxygen2 package helps with function documentation.
Documentation can be written in the form of comments #'
tags such as `@param` and `@description` structure the documentation of each function.

For an introcution to roxygen2, call
`vignette("roxygen2", package = "roxygen2")` at the R prompt.

Since roxygen2 version 6, markdown formating can be used in the documentation, 
by specifying the `@md` tag.

### Examples
Examples are crucial to demonstrate the use of a fonction. 
They can be specified in a roxygen block called `@examples`:

	#' @examples

Wrap the examples in donttest if you don't want R CMD check to test them at package building time.

	#' \donttest{d

It is also possible to wrap them in another statement called `dontrun`, but this is not recomended on CRAN
according to this [Stackoverflow question](https://stackoverflow.com/a/44997778/2641825).

### Vignettes

[Vignettes: long-form documentation](http://r-pkgs.had.co.nz/vignettes.html)

    devtools::use_vignette("my-vignette")

[Where to put package vignettes for CRAN submission](https://stackoverflow.com/questions/12325223/where-to-put-package-vignettes-for-cran-submission)

> "You put the .Rnw sources in vignettes/ as you did, but you missed out a critical step; don't check the source tree. The expected workflow is to build the source tarball and then check that tarball. Building the tarball will create the vignette PDF."

    R CMD build ../foo/pkg
    R CMD check ./pkg-0.4.tar.gz

Issues while building vignette for a packages:

* sh: 1: /usr/bin/texi2dvi: not found

    sudo apt-get install texinfo

* [Warning about "vignette without corresponding PDF/HTML"?](https://github.com/clonghurst/Res.plotter/issues/3)

> "Maybe you're running R CMD check using the directory name rather than the .tar.gz file?"

* [Inconsolata missing to build R vignette](https://stackoverflow.com/questions/34524357/inconsolata-missing-to-build-r-vignette)

> "Installing texlive-fonts-extra should take care of it."


R CMD checking data for non-ASCII characters found 179 marked UTF-8 strings 
No solution for this one but I guess it's ok since it concerns country names? 

## Unit tests
Back in R, add testing infrastructure:

    devtools::use_testthat()

When checking the package with R CMD CHECK, 
[How can I handle R CMD check “no visible binding for global variable”?](https://stackoverflow.com/questions/9439256/how-can-i-handle-r-cmd-check-no-visible-binding-for-global-variable-notes-when) 
These notes are caused by variables used with dplyr verbs and ggplot2 aesthetics.


## Continuous Integration
It is good to know if your package can be installed on a fresh system. 
Continuous integration systems make this possible each time you submit a modification to your repository. 
I have used travis-ci which is free for open github repositories. 
[Instructions to build an R project on travis](https://docs.travis-ci.com/user/languages/r/).
Unit tests are also run on travis, in addition to R CMD CHECK.

Package dependencies can be configured in a `.travis.yml`
file that is read by the travis machine performing the build.
For package that are not on Cran, it's possible to specify a dependency field under `r_github_packages`.

## Make file

* kbroman [minimal make](https://kbroman.org/minimal_make/)
* Cran [how to use make files with R CMD build](https://stackoverflow.com/questions/46741739/how-to-use-makefiles-with-r-cmd-build)

I copied the make file from the knitr package.

## Submitting to CRAN

[Submit package to CRAN](https://cran.r-project.org/submit.html)
The second page will ask to 

* Step 1 (Upload) the `*.tar.gz` file.
* Step 2 (Submission) review information from the package's DESCRIPTION file. 
* Step 3 (Confirmation)

The maintainer of this package has been sent an email to confirm the submission. After their confirmation the package will be passed to CRAN for review. 

# Differences between python and R

    l = c(1,2,3)
    s = l
    s[3]
    [1] 3
    s[3] = "a"
    s
    [1] "1" "2" "a"
    l
    [1] 1 2 3

Using the address function to see the address of these objects in memory
We can see that s and l share the same address. 
The address only changes when we asign something to s. 

    library(pryr)
    l = c(1,2,3)
    s = l
    address(l)
    [1] "0x316f718"
    address(s)
    [1] "0x316f718"
    s[3] = "a"
    address(s)
    [1] "0x36a7d30"
    s
    [1] "1" "2" "a"
    l
    [1] 1 2 3

Checking string objects

    bla = "qsdfmlkj"
    address(bla)
    [1] "0x38e5120"
    bli = bla
    address(bli)
    [1] "0x38e5120"
    bli = paste(bli, "sdf")
    address(bli)
    [1] "0x38d1d60"




TODO Compare to the same code in python
to see the difference between the above and passing by reference.



